from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional
from pathlib import Path
import os

from ai_town.retrieval.storage import load_manifest, get_index_path
from ai_town.retrieval.faiss_utils import load_faiss_index, search_index
from ai_town.retrieval.embedder import Embedder
from ai_town.config import DATA_DIR, DEFAULT_EMBED_METHOD, EMBED_MODEL_PATH, DEFAULT_GEN_MODEL, DEFAULT_ENABLE_OLLAMA, AI_TOWN_ALLOW_ONLINE_GEN

router = APIRouter()


class RAGRequest(BaseModel):
    query: str
    dataset: Optional[str] = None
    embed_method: Optional[str] = None
    model: Optional[str] = None
    topk: int = 3
    # generation model when Ollama not used
    gen_model: Optional[str] = None


class RAGResponse(BaseModel):
    answer: str
    evidence: list


def _dedupe_evidence(evidence, max_chars=2000):
    """å»é‡è¯æ®å¹¶é™åˆ¶æ€»å­—ç¬¦é•¿åº¦"""
    seen_texts = set()
    deduped = []
    total_chars = 0
    
    for e in evidence:
        text = e['text'][:400]  # æ¯ä¸ªè¯æ®æœ€å¤š400å­—ç¬¦
        text_hash = hash(text)
        if text_hash not in seen_texts and total_chars + len(text) <= max_chars:
            seen_texts.add(text_hash)
            deduped.append(e)
            total_chars += len(text)
    
    return deduped


def _build_optimized_prompt(query: str, evidence: list, gen_model: str) -> str:
    """
    æ ¹æ®ç”Ÿæˆæ¨¡å‹ç±»å‹æ„å»ºä¼˜åŒ–çš„ prompt
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        evidence: æ£€ç´¢åˆ°çš„è¯æ®åˆ—è¡¨
        gen_model: ç”Ÿæˆæ¨¡å‹åç§°æˆ–è·¯å¾„
    
    Returns:
        str: ä¼˜åŒ–çš„ prompt å­—ç¬¦ä¸²
    """
    low_name = str(gen_model).lower()
    is_flan_t5 = any(x in low_name for x in ('flan-t5', 'flan_t5'))
    is_t5 = 't5' in low_name
    is_seq2seq = any(x in low_name for x in ('t5', 'flan', 'bart', 'pegasus'))
    
    # ç®€åŒ–è¯æ®æ–‡æœ¬
    evidence_texts = []
    for i, e in enumerate(evidence[:3], 1):  # åªä½¿ç”¨å‰3ä¸ªè¯æ®
        text = e['text'].strip()
        # æ¸…ç†æ•°å­¦å…¬å¼å’Œç‰¹æ®Šç¬¦å·
        text = _clean_evidence_text(text)
        if len(text) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„ç‰‡æ®µ
            evidence_texts.append(text[:300])  # æ¯ä¸ªè¯æ®æœ€å¤š300å­—ç¬¦
    
    if not evidence_texts:
        return f"è¯·å›ç­”é—®é¢˜ï¼š{query}"
    
    if is_flan_t5:
        # FLAN-T5 ä¸“é—¨ä¼˜åŒ–çš„ prompt æ ¼å¼ - æ›´ç®€æ´ç›´æ¥
        evidence_str = " ".join(evidence_texts)[:600]  # è¿›ä¸€æ­¥é™åˆ¶é•¿åº¦
        # ç§»é™¤æ‰€æœ‰æ•°å­¦ç¬¦å·
        evidence_str = _clean_generated_text(evidence_str)
        
        # ä½¿ç”¨æ›´ç®€å•çš„ prompt æ ¼å¼
        prompt = f"æ ¹æ®ææ–™å›ç­”ï¼š{query}\nææ–™ï¼š{evidence_str}\nç­”æ¡ˆï¼š"
        
    elif is_t5:
        # æ™®é€š T5 çš„ç®€åŒ–æ ¼å¼
        evidence_str = " ".join(evidence_texts)[:600]
        prompt = f"ææ–™ï¼š{evidence_str}\né—®é¢˜ï¼š{query}\nç­”æ¡ˆï¼š"
        
    elif is_seq2seq:
        # å…¶ä»– seq2seq æ¨¡å‹
        evidence_str = "\n".join([f"- {text[:200]}" for text in evidence_texts])
        prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

{evidence_str}

é—®é¢˜ï¼š{query}
ç­”æ¡ˆï¼š"""
        
    else:
        # è‡ªå›å½’æ¨¡å‹ï¼ˆGPT ç±»ï¼‰çš„æ ¼å¼
        evidence_str = "\n".join([f"[{i}] {text[:250]}" for i, text in enumerate(evidence_texts, 1)])
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŸºäºæä¾›ææ–™å›ç­”é—®é¢˜çš„åŠ©ç†ã€‚è¯·æ ¹æ®ä»¥ä¸‹ææ–™ç®€æ´åœ°å›ç­”é—®é¢˜ã€‚

ææ–™ï¼š
{evidence_str}

é—®é¢˜ï¼š{query}

è¯·ç”¨1-3å¥è¯å›ç­”ï¼š"""
    
    return prompt


def _clean_evidence_text(text: str) -> str:
    """
    æ¸…ç†è¯æ®æ–‡æœ¬ï¼Œç§»é™¤æ•°å­¦å…¬å¼å’Œä¸å®Œæ•´çš„ç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡æè¿°
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        str: æ¸…ç†åçš„æ–‡æœ¬
    """
    import re
    
    # ç§»é™¤å•ç‹¬çš„æ•°å­¦ç¬¦å·å’Œå…¬å¼ç‰‡æ®µ
    text = re.sub(r'[ğœ¶ğ’Šğ’ğğ’™ğ’ƒğ’šğ‘±ğà·ğ’‚ğ’Œğ’ğ’‡]+', ' ', text)
    text = re.sub(r'[Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰Î©]+', ' ', text)
    
    # ç§»é™¤æ•°å­¦è¡¨è¾¾å¼
    text = re.sub(r'[ğ‘“ğ‘§ğ‘¥ğ‘¦ğ‘¤ğ‘]+\s*[=+\-*/]\s*[ğ‘“ğ‘§ğ‘¥ğ‘¦ğ‘¤ğ‘]+', ' ', text)
    text = re.sub(r'\b[a-zA-Z]\d*\s*[=+\-*/]\s*[a-zA-Z]\d*\b', ' ', text)
    text = re.sub(r'[=+\-*/]{2,}', ' ', text)
    
    # ç§»é™¤å•ç‹¬çš„ç¬¦å·å’Œæ•°å­¦ç»“æ„
    text = re.sub(r'[{}()[\]]+', ' ', text)
    text = re.sub(r'[â€¢Â·â–ªâ–«â– â–¡â—â—‹]+', ' ', text)
    text = re.sub(r'Î£\|ğ‘“', ' ', text)
    text = re.sub(r'à·\s*ğ’Š=ğŸ', ' ', text)
    
    # ç§»é™¤çº¯æ•°å­—è¡Œ
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
        line = line.strip()
        # ä¿ç•™åŒ…å«ä¸­æ–‡çš„è¡Œï¼Œæˆ–åŒ…å«å®Œæ•´è‹±æ–‡å•è¯çš„è¡Œ
        if any('\u4e00' <= char <= '\u9fff' for char in line) or \
           (len([w for w in line.split() if w.isalpha() and len(w) > 2]) > 0):
            # è¿›ä¸€æ­¥æ¸…ç†è¯¥è¡Œçš„æ•°å­¦ç¬¦å·
            line = re.sub(r'[ğœ¶ğ’Šğ’ğğ’™ğ’ƒğ’šğ‘±ğà·ğ’‚ğ’Œğ’ğ’Ÿğ’‡]+', ' ', line)
            if len(line.strip()) > 10:  # åªä¿ç•™æœ‰æ„ä¹‰é•¿åº¦çš„è¡Œ
                cleaned_lines.append(line.strip())
    
    text = ' '.join(cleaned_lines)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text


def _postprocess_generated_text(generated: str, prompt: str) -> str:
    """
    ä¼˜åŒ–åçš„ç”Ÿæˆæ–‡æœ¬åå¤„ç†å‡½æ•°ï¼Œä¸“é—¨å¤„ç† FLAN-T5 ç­‰æ¨¡å‹çš„è¾“å‡º
    """
    import re
    
    if not generated:
        return "æ— æ³•ç”Ÿæˆç­”æ¡ˆ"
    
    # æ¸…ç†ç”Ÿæˆæ–‡æœ¬
    generated = generated.strip()
    
    # æ¸…ç†æ•°å­¦ç¬¦å·å’Œä¸å®Œæ•´çš„å…¬å¼
    generated = _clean_generated_text(generated)
    
    # å¯¹äº text2text-generation æ¨¡å‹ï¼ˆå¦‚ T5ï¼‰ï¼Œé€šå¸¸ä¸ä¼šåŒ…å« prompt
    # ä½†å¯èƒ½åŒ…å«ä¸€äº›æ ¼å¼æ ‡è®°æˆ–é‡å¤å†…å®¹
    
    # å¦‚æœæ¨¡å‹æŠŠ prompt åŸæ ·å›æ˜¾ï¼Œå°è¯•åˆ é™¤ promptï¼ˆä¸»è¦é’ˆå¯¹ causal LMï¼‰
    if prompt and prompt in generated:
        generated = generated.split(prompt, 1)[-1].strip()
    
    # åˆ é™¤å¸¸è§çš„å¼€å¤´æ ‡è®°
    prefixes_to_remove = [
        "ç­”æ¡ˆ:", "å›ç­”:", "Answer:", "Response:", "ç­”:", "A:", 
        "æ ¹æ®ææ–™", "æ ¹æ®ä»¥ä¸Š", "åŸºäºææ–™", "ææ–™æ˜¾ç¤º", "æ–‡ä¸­æåˆ°"
    ]
    for prefix in prefixes_to_remove:
        if generated.startswith(prefix):
            generated = generated[len(prefix):].strip()
            # åˆ é™¤å¯èƒ½çš„å†’å·æˆ–é€—å·
            if generated.startswith((':', 'ï¼š', 'ï¼Œ', ',')):
                generated = generated[1:].strip()
    
    # åˆ é™¤é‡å¤çš„é—®é¢˜
    if "é—®é¢˜" in generated or "Question" in generated:
        parts = generated.split('\n')
        cleaned_parts = []
        for part in parts:
            if not any(marker in part.lower() for marker in ['é—®é¢˜', 'question', 'è¯æ®', 'evidence']):
                cleaned_parts.append(part.strip())
        if cleaned_parts:
            generated = ' '.join(cleaned_parts)
    
    # åˆ†ä¸ºå¥å­ï¼Œå¤„ç†ä¸å®Œæ•´çš„å¥å­
    sentences = re.split(r'[.ã€‚!ï¼?ï¼Ÿ;ï¼›]', generated)
    meaningful_sentences = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > 5 and not _is_incomplete_formula(sentence):
            meaningful_sentences.append(sentence)
    
    if meaningful_sentences:
        # å–å‰1-2ä¸ªå®Œæ•´çš„å¥å­
        result = 'ã€‚'.join(meaningful_sentences[:2])
        if not result.endswith(('ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?')):
            result += 'ã€‚'
        return result
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´å¥å­ï¼Œè¿”å›æ¸…ç†åçš„åŸæ–‡ï¼ˆå¦‚æœè¶³å¤Ÿé•¿ï¼‰
    if len(generated) > 10:
        # ç¡®ä¿ä»¥æ ‡ç‚¹ç»“å°¾
        if not generated.endswith(('ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?')):
            generated += 'ã€‚'
        return generated
    
    return "æ— æ³•æ ¹æ®æä¾›çš„ææ–™ç”Ÿæˆç­”æ¡ˆã€‚"


def _clean_generated_text(text: str) -> str:
    """
    æ¿€è¿›æ¸…ç†ç”Ÿæˆæ–‡æœ¬ä¸­çš„æ•°å­¦ç¬¦å·å’Œä¸å®Œæ•´ç‰‡æ®µ
    """
    import re
    
    # ç§»é™¤æ‰€æœ‰æ•°å­¦ç¬¦å·
    text = re.sub(r'[ğœ¶ğ’Šğ’ğğ’™ğ’ƒğ’šğ‘±ğà·ğ’‚ğ’Œğ’ğ’Ÿğ’‡ğ’Œğ’ğ’]+', '', text)
    text = re.sub(r'[Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰Î©]+', '', text)
    
    # ç§»é™¤æ•°å­¦è¡¨è¾¾å¼å’Œå…¬å¼
    text = re.sub(r'wmnk|wmn|ğ‘¤ğ‘šğ‘›|ğ‘ğ‘šğ‘›', '', text)
    text = re.sub(r'\b[a-zA-Z]\d*\s*[=+\-*/]\s*[a-zA-Z]\d*\b', '', text)
    text = re.sub(r'[=+\-*/]{2,}', '', text)
    text = re.sub(r'[=:+\-*/,ï¼Œã€‚ï¼š]{3,}', '', text)
    
    # ç§»é™¤å•ç‹¬çš„å­—æ¯å’Œæ•°å­—ç»„åˆï¼ˆå¦‚ k, m, nï¼‰
    text = re.sub(r'\b[a-zA-Z]\s*[,ï¼Œ]\s*[a-zA-Z]\s*[,ï¼Œ]\s*[a-zA-Z]\b', '', text)
    text = re.sub(r'\b[kmn]\s*[,ï¼Œ:ï¼š]', '', text)
    
    # ç§»é™¤å¤§æ‹¬å·ã€æ‹¬å·å’Œå…¶ä»–ç¬¦å·
    text = re.sub(r'[{}()[\]]+', ' ', text)
    text = re.sub(r'[â€¢Â·â–ªâ–«â– â–¡â—â—‹]+', ' ', text)
    text = re.sub(r'Î£\|ğ‘“|à·', ' ', text)
    
    # ç§»é™¤çº¯ç¬¦å·è¡Œ
    text = re.sub(r'^[^a-zA-Z\u4e00-\u9fff]*$', '', text, flags=re.MULTILINE)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[,ï¼Œã€‚ï¼š:]{2,}', 'ã€‚', text)
    
    return text.strip()


def _is_incomplete_formula(text: str) -> bool:
    """
    åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯ä¸å®Œæ•´çš„æ•°å­¦å…¬å¼
    """
    # å¦‚æœåŒ…å«å¤§é‡æ•°å­¦ç¬¦å·ï¼Œå¯èƒ½æ˜¯å…¬å¼
    math_chars = len([c for c in text if c in '+-*/=()[]{}Î±Î²Î³Î´ÎµÏ‰Ï€ÏƒÎ¼Î»Ï„Ï†Î¸Ïˆğœ¶ğ’Šğ’ğğ’™ğ’ƒğ’š'])
    total_chars = len(text)
    
    if total_chars > 0 and math_chars / total_chars > 0.3:
        return True
    
    # å¦‚æœä¸»è¦æ˜¯å•ä¸ªå­—æ¯å’Œæ•°å­—
    if len(text.split()) < 3 and any(c.isalpha() for c in text) and any(c.isdigit() for c in text):
        return True
    
    return False


def _extract_key_info_from_evidence(query: str, evidence: list) -> str:
    """
    å½“ç”Ÿæˆæ¨¡å‹å¤±è´¥æ—¶ï¼Œä»è¯æ®ä¸­æå–å…³é”®ä¿¡æ¯ä½œä¸º fallback ç­”æ¡ˆ
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        evidence: è¯æ®åˆ—è¡¨
        
    Returns:
        str: æå–çš„å…³é”®ä¿¡æ¯
    """
    if not evidence:
        return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
    
    # æ ¹æ®æŸ¥è¯¢ç±»å‹æå–ä¸åŒçš„å…³é”®ä¿¡æ¯
    query_lower = query.lower()
    
    key_sentences = []
    for e in evidence[:2]:  # åªçœ‹å‰ä¸¤ä¸ªæœ€ç›¸å…³çš„è¯æ®
        text = e['text']
        text = _clean_evidence_text(text)  # æ¸…ç†æ•°å­¦ç¬¦å·
        
        # æŒ‰å¥å­åˆ†å‰²
        import re
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:  # è·³è¿‡å¤ªçŸ­çš„å¥å­
                continue
                
            # æ ¹æ®æŸ¥è¯¢å…³é”®è¯åŒ¹é…ç›¸å…³å¥å­
            if any(keyword in sentence for keyword in ['äººå·¥æ™ºèƒ½', 'AI', 'å®šä¹‰']):
                if any(q_word in query for q_word in ['ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'äººå·¥æ™ºèƒ½']):
                    key_sentences.append(sentence)
            
            elif any(keyword in sentence for keyword in ['ç¥ç»ç½‘ç»œ', 'ç½‘ç»œ', 'ç®—æ³•', 'æ¨¡å‹']):
                if any(q_word in query for q_word in ['ç¥ç»ç½‘ç»œ', 'åŸç†', 'ä»€ä¹ˆæ˜¯']):
                    key_sentences.append(sentence)
                    
            elif any(keyword in sentence for keyword in ['æ·±åº¦å­¦ä¹ ', 'åº”ç”¨', 'é¢†åŸŸ', 'çªç ´']):
                if any(q_word in query for q_word in ['æ·±åº¦å­¦ä¹ ', 'åº”ç”¨', 'ç”¨é€”']):
                    key_sentences.append(sentence)
    
    if key_sentences:
        # å–æœ€ç›¸å…³çš„1-2ä¸ªå¥å­
        result = 'ã€‚'.join(key_sentences[:2])
        if not result.endswith('ã€‚'):
            result += 'ã€‚'
        return result
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…çš„å…³é”®å¥å­ï¼Œè¿”å›ç¬¬ä¸€ä¸ªè¯æ®çš„ç®€åŒ–ç‰ˆæœ¬
    first_evidence = evidence[0]['text']
    first_evidence = _clean_evidence_text(first_evidence)
    
    # æå–å‰100ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
    summary = first_evidence[:100].strip()
    if len(summary) > 20:
        if not summary.endswith('ã€‚'):
            summary += 'ã€‚'
        return f"æ ¹æ®èµ„æ–™æ˜¾ç¤ºï¼š{summary}"
    
    return "æŠ±æ­‰ï¼Œæ— æ³•ä»æä¾›çš„ææ–™ä¸­æå–åˆ°ç›¸å…³ä¿¡æ¯ã€‚"


@router.post('/rag')
def rag(req: RAGRequest):
    manifest = load_manifest()
    if not manifest:
        raise HTTPException(status_code=404, detail='No datasets found. Run ingest first')

    dataset = req.dataset
    if dataset is None:
        if len(manifest) == 1:
            dataset = list(manifest.keys())[0]
        else:
            raise HTTPException(status_code=400, detail=f'Multiple datasets available. Specify one. Available: {list(manifest.keys())}')

    entry = manifest.get(dataset)
    if not entry:
        raise HTTPException(status_code=404, detail=f'Dataset {dataset} not found')

    # ä½¿ç”¨ç»Ÿä¸€çš„ç´¢å¼•è·¯å¾„è®¿é—® API
    try:
        index_path = get_index_path(dataset)
        meta = entry.get('meta', [])
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))

    index = load_faiss_index(str(index_path))
    nb = getattr(index, 'ntotal', None)
    topk = req.topk
    if nb is not None and topk > nb:
        topk = int(nb)

    embed_method = req.embed_method or DEFAULT_EMBED_METHOD
    model = req.model or EMBED_MODEL_PATH

    emb = Embedder(method=embed_method, model_name=model)
    try:
        qv = emb.embed(req.query)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f'Embedding failed: {e}')

    if qv.ndim == 1:
        qv = qv.reshape(1, -1)

    D, I = search_index(index, qv.astype('float32'), k=topk)

    evidence = []
    for dist, idx in zip(D[0], I[0]):
        if int(idx) < 0:
            continue
        try:
            item = meta[int(idx)]
            text = item.get('text', '')
            source = item.get('source', '')
        except Exception:
            text = ''
            source = ''
        evidence.append({'idx': int(idx), 'distance': float(dist), 'source': source, 'text': text})

    # å»é‡å¹¶é™åˆ¶è¯æ®é•¿åº¦
    evidence = _dedupe_evidence(evidence, max_chars=2000)

    # æ„å»ºé’ˆå¯¹ä¸åŒæ¨¡å‹ä¼˜åŒ–çš„ prompt
    prompt = _build_optimized_prompt(req.query, evidence, req.gen_model or DEFAULT_GEN_MODEL)

    # å°è¯•ä½¿ç”¨ Ollamaï¼ˆè‹¥é…ç½®å…è®¸ï¼‰ï¼Œå¦åˆ™å›é€€åˆ°æœ¬åœ° transformers æ¨¡å‹ç”Ÿæˆ
    use_ollama = DEFAULT_ENABLE_OLLAMA
    try:
        from ai_town.core.ollama_client import OllamaClient
    except Exception:
        OllamaClient = None

    answer = None
    if use_ollama and OllamaClient is not None:
        client = OllamaClient(model=req.model or 'llama2')
        try:
            answer = client.chat(prompt)
        except Exception:
            answer = None

    if answer is None:
        # å›é€€åˆ°æœ¬åœ° transformers text-generation/text2text-generation
        try:
            from transformers import pipeline
        except Exception as e:
            raise HTTPException(status_code=500, detail=f'transformers not available for generation: {e}')
        gen_model = req.gen_model or DEFAULT_GEN_MODEL

        # å¦‚æœä¸å…è®¸åœ¨çº¿ä¸‹è½½ï¼Œç¡®ä¿æœ¬åœ°æ¨¡å‹è·¯å¾„å­˜åœ¨
        if not AI_TOWN_ALLOW_ONLINE_GEN:
            # å½“ gen_model çœ‹èµ·æ¥åƒæœ¬åœ°è·¯å¾„æ—¶ï¼Œæ£€æŸ¥å…¶å­˜åœ¨æ€§
            try:
                gm_path = Path(gen_model)
                if not gm_path.exists():
                    raise HTTPException(status_code=400, detail=f'Local generation model not found at {gen_model}. Set AI_TOWN_ALLOW_ONLINE_GEN=true to allow online download or save the model to this path.')
            except HTTPException:
                raise
            except Exception:
                # è‹¥ gen_model ä¸æ˜¯è·¯å¾„æ ¼å¼ï¼Œå…è®¸ç»§ç»­ï¼ˆå¯èƒ½æ˜¯ HF name but online disabledï¼‰
                raise HTTPException(status_code=400, detail=f'Generation model {gen_model} not found locally and online download is disabled.')

        # é€‰æ‹© pipeline ä»»åŠ¡ï¼šè‹¥æ˜¯ encoder-decoderï¼ˆt5/flanï¼‰ä½¿ç”¨ text2text-generation
        task = 'text-generation'
        low_name = str(gen_model).lower()
        is_seq2seq = any(x in low_name for x in ('t5', 'flan', 'bart', 'pegasus'))
        if is_seq2seq:
            task = 'text2text-generation'
        
        # ä½¿ç”¨ä¼˜åŒ–åçš„ promptï¼ˆå·²ç»æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œäº†ä¼˜åŒ–ï¼‰
        gen_prompt = prompt
            
        # å°è¯•ä½¿ç”¨ GPU device 0ï¼Œå¦åˆ™é»˜è®¤
        try:
            generator = pipeline(task, model=gen_model, device=0)
            # å¯¹äº FLAN-T5ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ç”Ÿæˆå‚æ•°
            if is_seq2seq:
                out = generator(gen_prompt, max_new_tokens=50, do_sample=False, 
                               temperature=0.0, truncation=True, 
                               no_repeat_ngram_size=3)
            else:
                out = generator(gen_prompt, max_new_tokens=128, do_sample=False, 
                               temperature=0.0, truncation=True)
        except Exception:
            generator = pipeline(task, model=gen_model)
            if is_seq2seq:
                out = generator(gen_prompt, max_new_tokens=50, do_sample=False, 
                               temperature=0.0, truncation=True,
                               no_repeat_ngram_size=3)
            else:
                out = generator(gen_prompt, max_new_tokens=128, do_sample=False, 
                               temperature=0.0, truncation=True)
        # æå–ç”Ÿæˆç»“æœ
        if isinstance(out, list) and len(out) > 0:
            # text2text returns 'generated_text' or 'summary_text' depending on pipeline
            first = out[0]
            if isinstance(first, dict):
                raw = first.get('generated_text') or first.get('summary_text') or str(first)
            else:
                raw = str(first)
        else:
            raw = str(out)

        # åå¤„ç†ï¼šå‰¥ç¦» prompt å¹¶æå–é¦–ä¸ªåˆç†ç­”æ¡ˆæ®µ
        answer = _postprocess_generated_text(raw, prompt)
        
        # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œå°è¯•ä»è¯æ®ä¸­æå–å…³é”®ä¿¡æ¯ä½œä¸º fallback
        if not answer or answer == "æ— æ³•æ ¹æ®æä¾›çš„ææ–™ç”Ÿæˆç­”æ¡ˆã€‚" or len(answer) < 10:
            answer = _extract_key_info_from_evidence(req.query, evidence)

    return {'answer': answer, 'evidence': evidence}

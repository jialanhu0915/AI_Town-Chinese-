{
  "1_神经网络_原理与实现": {
    "vectors": "1_神经网络_原理与实现_vectors.npy",
    "index_original": "1_神经网络_原理与实现.index",
    "index_safe": "1_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0_5d37554e.index",
    "meta": [
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 0,
        "text": "神经网络 ——原理与实现  人工智能与深度学习  神经网络原理概述  神经网络的搭建与训练  基于Keras实现神经网络应用 1. 人工智能与深度学习 人工智能没有标准的定义，不同的人群不同的阶段给以不同的理解。 人工智能的定义  公众视角：机器可完成人们认为机器不能胜任的事。  早期定义：是与人类思考方式相似的计算机程序，遵循逻辑学的基本规律进 行推理、运算、归纳。  流行定义：是和人类行为相似的计算机程序。  发展趋势定义：会学习的计算机程序，“无学习，不AI”。 人工智能就是根据对环境的感 知做出合 理的行动，并获得最 大收益的 计算 机程序。 图灵测试 人工智能发展历程：严冬和热潮交替 第三次热潮的推动力：AI三要素 基于大数据、云计算和深度学习算法。 深度学习突破性进展  基于 ImageNet数据 集(1400多万幅图片 ，涵盖 2万多个 类别 )视觉 识别挑战赛 （ILSVRC），2015年图像分类能力超过人眼。  2017 AlphaGo Zero实现自我学习，在围棋对弈上全面超过人类。 深度学习在计算机视觉、自然语言处理等领域取得突破进展。 战胜世界冠军团队 Alphago与柯洁对弈 ILSVR图片分类成绩 大语言模型 大语言模型是一种由包含数百亿以上参数的深度神经网络构建的语言模型。 通常使用自监督学习方法通过大量无标注文本进行训练，获得人类语言的统计规 律，进而能够预测和生成自然语言文本、程序代码等各类内容，并具有一定推理 能力，还能泛化处理语言生成以外的其他任务，如音乐、绘画创作等。 2022年12月OpenAI发布ChatGPT，它使用海量数据学习训练 ，并用人类 的反馈信息强化学习。具有丰富知识、文本生成和思维链推理能力。 大语言模型训练流程 BM已具备基本的知 识、理解及生成能力 数据 算法 算力 模型 数千亿级百科、网 页、电子书、代码 模型预训练 基于Transformers 的深度学习算法 1000+GPUs 月级别训练 预训练模型 Base Model SFT可更好地适应问答 任务 数万人工标注或优 质问答数据 有监督指令微调 预训练模型有监督 指令微调SFT 1-100GPUs 天级别训练 微调模型 SFT RM对SFT模型的多个 输出评价，辅助RL 百万标注对 训练奖励模型 二分类算法 预测奖励 1-10"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 1,
        "text": "（4）进行预测 一元线性回归 通过拟合一个目标变量和一个特征之间最佳的线性关系来预测目标变量值。 𝒇 𝒙 = 𝝎𝒙 + 𝒃 即给定数据集：𝑫 = { 𝒙𝟏, 𝒚𝟏 , 𝒙𝟐, 𝒚𝟐 , … , 𝒙𝒎, 𝒚𝒎 } 试图学得： 使得： 𝒇 𝒙i ≈ 𝒚𝒊 特征：描述数据的输入变量𝒙 ，如:父母平均身高 标签：输入变量对应的已知真实值𝒚 ，如：子女平均身高 样本: 指数据的特定实例, 有标签样本｛特征,标签｝，如：{𝒙𝟏, 𝒚𝟏}，用于训练模型 无标签样本｛特征,？ ｝，如｛ 𝒙𝒌 ，?｝,用于预测的输入 模型：含有参数的表达式或结构，参数值是通过学习得到的 训练:通过学习有标签样本来确定模型的所有参数的理想值 序 号 父母平 均身高 子女平 均身高 1 68.5 68.6 2 72.8 73.0 3 70.8 70.9 4 … … 5 65.5 ? y = 33.73 + 0.516x y = 33.73 + 0.516x 𝑦: 子女平均身高 𝑥:父母平均身高 从标注数据集 中学习得到 (监督学习) 例如：以下一元线性回归模型可以利用父母平均身高来预测其子女平均身高。 给定数据集：𝑫 = { 68.5,68.6 , 72.8,73.0 , … … } 试图学得： 使得： 𝒇 𝒙i ≈ 𝒚𝒊 序 号 父母平 均身高 子女平 均身高 1 68.5 68.6 2 72.8 73.0 3 70.8 70.9 4 … … 模型实际意义解释：基础身高33.73英寸，父母平均身高每增加1英寸， 子女平均身高增加0.516英寸。 𝒇 𝒙 = 𝝎𝒙 + 𝒃 𝒙 𝒚 参数b 参数𝝎 一元线性回归模型的参数学习 • 有监督学习过程 基于给定的数据集(𝒙𝒊, 𝒚𝒊 ), 𝒊 = 𝟏 … 𝒎，其中𝒙𝒊 是样本特征值，𝒚𝒊 是样本标签值， 获得𝒇 𝒙 模型的参数{𝜔, 𝑏}，𝜔: 回归系数，𝑏: 截距，使得模型在数据集上预测的误 差最小。 𝒚 ≈ 𝒇 𝒙 = 𝝎𝒙 + 𝒃 损失(cost)：预测误差也称为损失(或代价)，表示对于单个样本而言模型预测的准确程度。 损失函数(Cost Function)：用来从总体描述模型的预测值𝒇 𝒙 与真实值𝒚不一致程度的非负实值函数。 针对不同的问题，可以定义相应的损失函数。 左侧模型的损失较大 右侧模型的损失较小 训练的目标是根据所有样 本找到一组线性模型参数"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 2,
        "text": "𝒙 = 𝝎𝒙 + 𝒃 损失(cost)：预测误差也称为损失(或代价)，表示对于单个样本而言模型预测的准确程度。 损失函数(Cost Function)：用来从总体描述模型的预测值𝒇 𝒙 与真实值𝒚不一致程度的非负实值函数。 针对不同的问题，可以定义相应的损失函数。 左侧模型的损失较大 右侧模型的损失较小 训练的目标是根据所有样 本找到一组线性模型参数， 使得损失函数值最小 例如：平均绝对误差函数 𝑱 = σ𝒊=𝟏 𝒎 𝒇 𝒙𝒊 − 𝒚𝒊 𝒎 = σ𝒊=𝟏 𝒎 𝝎𝒙𝒊 + 𝒃 − 𝒚𝒊 𝒎 均方误差函数 𝑱 = σ𝒊=𝟏 𝒎 (𝒇 𝒙𝒊 −𝒚𝒊)𝟐 𝒎 = σ𝒊=𝟏 𝒎 (𝝎𝒙𝒊+𝒃−𝒚𝒊)𝟐 𝒎 统计学上的“最小二乘 法” 一元线性回归模型的参数学习——正规方程法 𝑱 = 𝟏 𝒎 ෍ 𝒊=𝟏 𝒎 (𝒇 𝒙𝒊 − 𝒚𝒊)𝟐= 𝟏 𝒎 ෍ 𝒊=𝟏 𝒎 (𝝎𝒙𝒊 + 𝒃 − 𝒚𝒊)𝟐 要使函数𝑱具有最小值，可对参数𝜔和b分别求导，令其导数值 为零(到达最小极值)，求解建立的这个方程组，即可得到参数 𝜔和b的值。经过推导(过程略)，计算式如下： 1）天才算法，直接求解！只要样本数大于参 数个数，即可求得任意多元线性回归参数。 2）但参数多时(如大于1万)计算量很大！ 一元线性回归模型的参数学习——梯度下降法 梯度下降是一种迭代法。计算过程就是沿梯度下降的方向求解极小值。 开始时随机选择一个参数组合（𝝎, 𝒃）计算损失函数值，然后寻找下一个能 让损失函数下降最多的参数组合。持续迭代，直到找到一个局部最小值。 𝝎 = 𝝎 − 𝜶 𝝏𝑱 𝝏𝝎 𝒃 = 𝒃 − 𝜶 𝝏𝑱 𝝏𝒃 Repeat Until 收敛 { } 𝜶: 学习率 • 沿着损失函数下降最快的方向迈出步子的大小 • 𝜶过小，迭代次数高；𝜶过大，可能越过局部最小值导致无法收敛 • 一般选0.01、0.03 、0.1 、0. 3、1 、3 、10 𝑱 = 𝟏 𝒎 ෍ 𝒊=𝟏 𝒎 (𝒇 𝒙𝒊 − 𝒚𝒊)𝟐= 𝟏 𝒎 ෍ 𝒊=𝟏 𝒎 (𝝎𝒙𝒊 + 𝒃 − 𝒚𝒊)𝟐 勤奋努力，试错改进！ 不怕参数大！ 一元线性回归模型的参数学习——批量梯度下降法 𝝎 = 𝝎 − 𝜶 𝝏𝑱 𝝏𝝎 = 𝝎 − 𝜶 2 m ෍ 𝒊=𝟏 𝒎 𝝎𝒙𝒊 + 𝒃 − 𝒚𝒊 𝒃 = 𝒃 − 𝜶 𝝏𝑱 𝝏𝒃 = 𝒃 − 𝜶 2 m ෍ 𝒊=𝟏 𝒎 （ 𝝎𝒙𝒊 +"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 3,
        "text": "𝜶 2 m ෍ 𝒊=𝟏 𝒎 𝝎𝒙𝒊 + 𝒃 − 𝒚𝒊 𝒃 = 𝒃 − 𝜶 𝝏𝑱 𝝏𝒃 = 𝒃 − 𝜶 2 m ෍ 𝒊=𝟏 𝒎 （ 𝝎𝒙𝒊 + 𝒃 − 𝒚𝒊 𝒙𝒊） Repeat Until 收敛 { } 梯度下降的每一步中，计 算都使用所有训练样本 可以绘制迭代次数和损失函数 的折线图来观察算法收敛趋势。 𝑱 = 𝟏 𝒎 ෍ 𝒊=𝟏 𝒎 (𝒇 𝒙𝒊 − 𝒚𝒊)𝟐= 𝟏 𝒎 ෍ 𝒊=𝟏 𝒎 (𝝎𝒙𝒊 + 𝒃 − 𝒚𝒊)𝟐 损失函数值的变化趋势 2. 神经网络原理概述 神经重接实验及各种人类行为引起科学家猜想：大脑可以用一种 算法处理各种不同问题，只要接入传感信息，大脑就能学会处理它？！ 神经网络是人们为了模仿大脑产生的一种算法。 人脑工作原理猜想 人脑及神经元 人脑包含800亿个神经元，这些神经元类似 一个个小的处理单元， 它们按照某种方式连接， 接受外部刺激，做出响应处理的过 程也就是大 脑的对信息处理的过程。 神经元(生物模型)  树突：神经元的输入通道 ，接收其 他神经元电信号  细胞体：处理这些信号，细胞核达到 某种“兴奋”状态，就发出信号  轴突和突触：将处理过的信号传递 到下一个神经元  输入信号： 𝑥0, 𝑥1, … 𝑥𝑛  处理单元 ：对每个输入信号进行加 权处理 (𝜔0, 𝜔1, … 𝜔𝑛 )确定其强度 ； 然后求和确 定组合 效果，再加上截 距b；通过激励函数达到一定的阈 值输出  输出信号： y = 𝑓(σ𝑖=0 𝑛 𝜔𝑖 𝑥𝑖 + 𝑏) 人工神经元(数学模型)  生物神经元的启示 ① 每个神经元都是一个多输入单输出的信号处理单元 ② 神经元具有阈值特性 人工神经元的模型描述 𝒚 = 𝒇 𝝎𝟎𝒙𝟎 + 𝝎𝟏𝒙𝟏 … + 𝝎𝒏𝒙𝒏 + 𝒃 其中： 𝑥0, 𝑥1, … 𝑥𝑛：输入信号 𝜔0, 𝜔1, … 𝜔𝑛：各输入信号的权重 b：是阈值，也称偏置项或截距 𝒇 𝒛 ：激活函数，是非线性函数 1 o Sigmoid函数 𝑓 𝑧 = 1 1 + 𝑒−𝑧 当激活函数选Sigmoid函数时，单个神 经元就相当于逻辑回归模型！ 𝑤1 𝑤2 𝑤3 𝑥1 𝑥2 𝑥3 Σ|𝑓 𝑏 𝒚 𝑓𝑤 𝑥 = 1 1+𝑒−(𝑤1𝑥1+𝑤2𝑥2+⋯+𝑤𝑛𝑥𝑛+𝑏) 人工神经网络(Artificial Neural Netork,即ANN) 模仿神经元在人脑 中的结构"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 4,
        "text": "𝒛 ：激活函数，是非线性函数 1 o Sigmoid函数 𝑓 𝑧 = 1 1 + 𝑒−𝑧 当激活函数选Sigmoid函数时，单个神 经元就相当于逻辑回归模型！ 𝑤1 𝑤2 𝑤3 𝑥1 𝑥2 𝑥3 Σ|𝑓 𝑏 𝒚 𝑓𝑤 𝑥 = 1 1+𝑒−(𝑤1𝑥1+𝑤2𝑥2+⋯+𝑤𝑛𝑥𝑛+𝑏) 人工神经网络(Artificial Neural Netork,即ANN) 模仿神经元在人脑 中的结构连接。一个网络可以由几个甚至几 百万个人 工神经元构成，这些神经元排列在 一系列的层中，每个层之间彼此相连。 一个完整的神经网络由 一层输入层、多层隐藏层、一层输出层构成。 每 层神经元 与下一层 神经元全互联 ，同层神经元之间 不存在连接。其中输入 层接收外界输入 ，隐层和输出层神经元对信号进行加工 ，最终结果由输出 层神经元输出。 上图被称为双隐层神经网络，也称为三层网络(因为输入层没有处理能力)， 共有12个神经元。 神经网络中的神经元之间如何连接？ 𝑤11 𝑤21 𝑤31 𝑥1 𝑥2 𝑥3 𝑎1 𝑎2 𝑤12𝑤22 𝑤32 Σ|𝑓 Σ|𝑓 Σ|𝑓 𝑤11 𝑤21 𝑤12 𝑤22 𝑤13𝑤23 𝑦1 𝑦2 𝑦3 Σ 𝑓 Σ 𝑓 神经元之间是多对多的关系：  输入：来自多个上层的神经元  输出 ：到多个下层神经元 为方便绘图，这 里将单个神经元 的截距b省略 单层全连接网络 Σ|𝑓 𝑤11 𝑤21 𝑤31 𝑥1 𝑥2 𝑥3 Σ|𝑓 y1 𝑦2 𝑤12𝑤22 𝑤32 多层全连接网络 𝑤1 𝑤2 𝑤3 𝑥1 𝑥2 𝑥3 Σ |𝑓 𝒚 也称感知机网络（Perceptron Networks）  无隐藏层，只有输入层/输出层  可以处理线性问题  至少有一个隐藏层  可以处理非线性问题 神经网络的参数 𝑤11 𝑤21 𝑤31 𝑥1 𝑥2 𝑥3 𝑎1 𝑎2 𝑤12𝑤22 𝑤32 Σ|𝑓 Σ|𝑓 Σ|𝑓 𝑤11 𝑤21 𝑤12 𝑤22 𝑤13𝑤23 𝑦1 𝑦2 𝑦3 Σ 𝑓 Σ 𝑓 多层全连接网络 𝑎1 = 𝑓(𝒘𝟏𝟏 𝟏 𝒙𝟏 + 𝒘𝟐𝟏 𝟏 𝒙𝟐 + 𝒘𝟑𝟏 𝟏 𝒙𝟑 + 𝑏11) 𝑎2 = 𝑓(𝒘𝟏𝟐 𝟏 𝒙𝟏 + 𝒘𝟐𝟐 𝟏 𝒙𝟐 + 𝒘𝟑𝟐 𝟏 𝒙𝟑 + 𝑏12) 𝑦1 = 𝑓(𝒘𝟏𝟏 𝟐 𝒂𝟏 + 𝒘𝟐𝟏 𝟐 𝒂𝟐 + 𝑏21) 𝑦2 = 𝑓(𝒘𝟏𝟐 𝟐 𝒂𝟏 + 𝒘𝟐𝟐 𝟐"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 5,
        "text": "+ 𝒘𝟐𝟏 𝟏 𝒙𝟐 + 𝒘𝟑𝟏 𝟏 𝒙𝟑 + 𝑏11) 𝑎2 = 𝑓(𝒘𝟏𝟐 𝟏 𝒙𝟏 + 𝒘𝟐𝟐 𝟏 𝒙𝟐 + 𝒘𝟑𝟐 𝟏 𝒙𝟑 + 𝑏12) 𝑦1 = 𝑓(𝒘𝟏𝟏 𝟐 𝒂𝟏 + 𝒘𝟐𝟏 𝟐 𝒂𝟐 + 𝑏21) 𝑦2 = 𝑓(𝒘𝟏𝟐 𝟐 𝒂𝟏 + 𝒘𝟐𝟐 𝟐 𝒂𝟐 + 𝑏22) 𝑦3 = 𝑓(𝒘𝟏𝟑 𝟐 𝒂𝟏 + 𝒘𝟐𝟑 𝟐 𝒂𝟐 + 𝑏23) 权重参数𝑤𝑚𝑛𝑘 :𝑘表示第几层, 𝑚表示第几个输入， 𝑛 表示 第几个神经元，共12个 阈值𝑏𝑚𝑛 （也称截距或偏置项）: 共有5个 一个网络可以包含几百万个神经元， 多层全连接神经网络参数量巨大! 激活函数  为什么引入激活函数  增强网络的表达能力，需要激活函数来将线性函数->非线性函数；否则多层 网络连接和单层神经网络本质上就一样了。  非线性的激活函数需要有连续性。因为连续非线性激活函数是可导的，才可 以用最优化的方法来求解 𝑤11 𝑤21 𝑤31 𝑥1 𝑥2 𝑥3 𝑎1 𝑎2 𝑤12𝑤22 𝑤32 Σ|𝑓 Σ|𝑓 Σ|𝑓 𝑤11 𝑤21 𝑤12 𝑤22 𝑤13𝑤23 𝑦1 𝑦2 𝑦3 Σ 𝑓 Σ 𝑓 𝑎1 = 𝑓(𝒘𝟏𝟏 𝟏 𝒙𝟏 + 𝒘𝟐𝟏 𝟏 𝒙𝟐 + 𝒘𝟑𝟏 𝟏 𝒙𝟑 + 𝑏11) 𝑎2 = 𝑓(𝒘𝟏𝟐 𝟏 𝒙𝟏 + 𝒘𝟐𝟐 𝟏 𝒙𝟐 + 𝒘𝟑𝟐 𝟏 𝒙𝟑 + 𝑏12) 𝑦1 = 𝑓(𝒘𝟏𝟏 𝟐 𝒂𝟏 + 𝒘𝟐𝟏 𝟐 𝒂𝟐 + 𝑏21) 𝑦2 = 𝑓(𝒘𝟏𝟐 𝟐 𝒂𝟏 + 𝒘𝟐𝟐 𝟐 𝒂𝟐 + 𝑏22) 𝑦3 = 𝑓(𝒘𝟏𝟑 𝟐 𝒂𝟏 + 𝒘𝟐𝟑 𝟐 𝒂𝟐 + 𝑏23) 常用激活函数 1 o Sigmoid函数 双曲正切(tanh)函数 𝜎 𝑧 = 1 1 + 𝑒−𝑧 ሖ𝜎 𝑧 = 𝜎(𝑧)(1 − 𝜎(𝑧)) tanh 𝑧 = 2𝜎 2𝑥 − 1 = 𝑒𝑧 − 𝑒−𝑧 𝑒𝑧 + 𝑒−𝑧 ሖtanh(𝑧) ="
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 6,
        "text": "𝒘𝟐𝟐 𝟐 𝒂𝟐 + 𝑏22) 𝑦3 = 𝑓(𝒘𝟏𝟑 𝟐 𝒂𝟏 + 𝒘𝟐𝟑 𝟐 𝒂𝟐 + 𝑏23) 常用激活函数 1 o Sigmoid函数 双曲正切(tanh)函数 𝜎 𝑧 = 1 1 + 𝑒−𝑧 ሖ𝜎 𝑧 = 𝜎(𝑧)(1 − 𝜎(𝑧)) tanh 𝑧 = 2𝜎 2𝑥 − 1 = 𝑒𝑧 − 𝑒−𝑧 𝑒𝑧 + 𝑒−𝑧 ሖtanh(𝑧) = 1 − tanh(𝑥)2 ReLU函数 𝑟𝑒𝑙𝑢 𝑧 = max 0, 𝑧 ሖ𝑟𝑒𝑙𝑢 𝑧 = ቊ1, 𝑧 > 0 0, 𝑧 < 0 映射到(0,1)区间，二分类常用 计算量大，易出现梯度消失 映射到(-1,1)区间，常用在RNN 易出现梯度消失 线性运算，效率极高，最常用 1 o -1 o 前馈型神经网络 (FeedforwardNetwork) 单向多层结构，即各神经元从输入 层开始，只接收上一层的输出并输 出到下一层，直至输出层，整个网 络拓扑 中无反馈回路。常用于图像 识别、检测、分割。 反馈型神经网络 (Recurrent Network) 又成自联想记忆网络， 是一种从输 出到输入具有反馈连接的神经网络，当 前的结果受到先前所有的结果的影响 , 是一种反馈动力学系统。 常用于语音、 文本处理、问答系统等 。 3. 神经网络的搭建和训练 搭建神经网络 假定要搭建一个神经网络实现手写数字的识别 。需定义网络结构：  输入层神经元个数： 784（每个神经元对应一个像素值 ）。  隐藏层层数及每层神经元个数：根据经验和直觉定义，可以通过欠 拟合、过拟合等表现调整。例如隐层 1选128，隐层2选256。  输出层神经元个数： 分类数，本例选10，因为有10个类别的数字  激活函数：输入层和隐藏层选择 Relu函数或Sigmoid函数，输出层 因为是多分类，选择 softmax函数。 图像分辨率28*28， 每图像784个像素 每一输出代表对应 某一标签的概率 Softmax函数  softmax函数，又称归一化指数函数，是二分类函数sigmoid在多分 类上的推广，目的是将多分类的结果以概率的形式展现出来。 𝑆(𝑦𝑖) = 𝑒𝑦𝑖 σ𝑗 𝑒𝑦𝑗 概率分布：1>𝑆 𝑦𝑖 > 0, σ𝑖 𝑆 𝑦𝑖 = 1 𝑦𝑖表示在第i个分类上的输出结果  分类方式：得分最高的那一类即为最后输入对应的类别。 训练神经网络——"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 7,
        "text": "= {权重参数𝑤𝑚𝑛𝑘 , 阈值参数𝑏𝑚𝑛}, 其中：k表示第几层, m表示第几个输入, n 表示第几个神经元  枚举所有可能的取值 参数个数巨大  寻找模型最佳参数使损失函数值最小 例如：语音识别模型有 8层， 每层1000神经元 L+1层 …… L层 …… 106 每层权重参数 1000神经元 1000神经元 训练神经网络——损失函数定义  单个样本的损失函数：  平方损失函数 衡量两个距离的差异，常用于回归任务 𝐿𝑜𝑠𝑠 𝑦, ො𝑦 = (𝑦 − ො𝑦)2, ෝ𝑦 = 𝑓(𝑥, 𝑤)  交叉熵损失函数 衡量两个概率分布的差异，常用于分类任务 𝐿𝑜𝑠𝑠 𝑦, ො𝑦 = − ෍ 𝑖=1 𝐶 𝑦𝑖 ∗ 𝑙𝑜𝑔 ෝ𝑦𝑖 , ෝ𝑦𝑖 = 𝑓𝑖(𝑥, 𝑤)  整体样本的损失函数：从数据集的整体来看损失情况。  优化目标：模型在训练集上的平均差异最小化 𝐿 𝑤 = 1 𝑚 σ𝑚 𝐿𝑜𝑠𝑠 𝑓(𝑥, 𝑤) , ො𝑦 其中m为样本个数 损失函数用来估量模型预测值与真实值之间的差距。损失函数给出的差距 越小，则模型鲁棒性就越好。损失函数的设计要根据具体任务的特点。 训练神经网络——参数学习 采用梯度下降算法求解损失函数的最小值，从而获得最优参数组合。 w 传统算法流程： 选择一个初始值w Repeat 对w的每一份量计算梯度值 Τ𝜕𝐿 𝜕𝑤𝑗 𝑤𝑗 Τ← 𝑤𝑗 − 𝜂𝜕𝐿 𝜕𝑤𝑗 Until 直到 Τ𝜕L 𝜕w 非常小（即更新很小） Τ−𝜂𝜕𝐿 𝜕𝑤 η 是“学习率” 神经网络参数𝑤 = {𝑤𝑚𝑛𝑘 , 𝑏𝑚𝑛} 传统算法计算量 巨大，一度使神 经网络陷入低潮！ 𝐿 训练神经网络——误差反向传播 1986年以后，误差反向传播算法BP(error back propagation, BP)得 到关注，解决了对参数逐一求偏导的效率低下问题，为梯度下降算 法提供了高效率的实现方法。 基本思想：由于神经网络涉及多层神经元，将输出层误差逐层 反向传播给各隐藏层进行参数更新。 1. 通过正向传播计算得到输出值，并计算误差。 2. 通过误差反向传播逐层计算各节点输入参数 的梯度(即误差)，并根据梯度修改相应参数值。 将误差从后向前传递，分摊给各层所有单元，从而 获得各层单元所产生的误差，进而依据这个误差来 让各层单元负起各自责任、修正各单元参数。 一次前向传播和一次反向传播就可以"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 8,
        "text": "输出层 𝑤1 𝑤2 𝑤3 w4 𝑤5 𝑤6 𝑤7 𝑤8 𝑥1 𝑥2 ℎ1 ℎ2 𝑜1 𝑜2 𝒚𝟏 𝒚𝟐 𝛿o1 𝛿o2 𝛿5 𝛿6 𝛿7 𝛿8 𝛿1 𝛿2 𝛿3 𝛿4 前向传播：计算输出值 反向传播：计算梯度 前馈神经网络的误差反向传播 𝐼𝑛=𝑤m ∗ 𝑎+𝑤n ∗ 𝑏 𝑂𝑢𝑡= 𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝐼𝑛) 𝑤8 𝐼𝑛 𝑂𝑢𝑡 𝑤n 𝑎 𝑏 𝑤m 其中每个神经元的结构如下，包含两部分操作：  In操作：对输入加权求和  Out操作：对In进行激活函数非线性变换 如下结构的2层神经网络，图中𝑤𝑖为连接权重，Error表示损失。 输入层 隐藏层 输出层 𝑥1 𝑥2 ℎ1 ℎ2 𝑜1 𝑜2 𝑤1 𝑤2 𝑤3 w4 𝑤5 𝑤6 𝑤7 𝑤8 𝐼𝑛ℎ1 = 𝑤1 ∗ 𝑥1 + 𝑤3 ∗𝑥2 ℎ1 = 𝑂𝑢𝑡ℎ1 = 𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝐼𝑛ℎ1 ) 𝑥1 𝑥2 ℎ1 ℎ2 𝑜1 𝑜2 𝑤1 𝑤2 𝑤3 𝑤5 𝑤6 𝑤7 𝑤8w 4 前向传播——输出计算 𝐼𝑛ℎ2 = 𝑤2 ∗ 𝑥1 + 𝑤4 ∗𝑥2 ℎ2 = 𝑂𝑢𝑡ℎ2 = 𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝐼𝑛ℎ2) 𝐼𝑛𝑜1 = 𝑤5 ∗ ℎ1 + 𝑤7 ∗ℎ2 𝑜1 = 𝑂𝑢𝑡𝑜1 = 𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝐼𝑛𝑜1) 𝐼𝑛𝑜2 = 𝑤6 ∗ ℎ1 + 𝑤8 ∗ℎ2 𝑜2 = 𝑂𝑢𝑡𝑜2 = 𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝐼𝑛𝑜2) E𝑟𝑟𝑜𝑟 = 1 2 ෍ 𝑖=1 2 (𝑜𝑖 − 𝑦𝑖)2 𝒚𝟏 𝒚𝟐 式中1/2是为了后面求导约简 方便，不影响网络计算效果。 1. 计算𝑤5~ 𝑤8的梯度(以𝑤5为例) 𝑥1 𝑥2 ℎ1 ℎ2 𝑜1 𝑜2 𝑤1 𝑤3 𝑤5 𝑤6 𝑤7 𝑤8 where, w4 反向传播——梯度计算 链式法则：两个函数组合起来的复合函数，导 数等于里函数带入外函数值的导数乘以里函数的 导数。即：y = 𝑓(𝑔 𝑥 ) , 若记𝑦 = 𝑓 𝑢 , u = g(x), 则有： dy 𝑑𝑥 = dy 𝑑𝑢 ∗ d𝑢 𝑑𝑥 𝑤 𝐼𝑛 𝑂𝑢𝑡 𝑤n 𝑎 𝑤m b 𝛿5"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 9,
        "text": "计算𝑤5~ 𝑤8的梯度(以𝑤5为例) 𝑥1 𝑥2 ℎ1 ℎ2 𝑜1 𝑜2 𝑤1 𝑤3 𝑤5 𝑤6 𝑤7 𝑤8 where, w4 反向传播——梯度计算 链式法则：两个函数组合起来的复合函数，导 数等于里函数带入外函数值的导数乘以里函数的 导数。即：y = 𝑓(𝑔 𝑥 ) , 若记𝑦 = 𝑓 𝑢 , u = g(x), 则有： dy 𝑑𝑥 = dy 𝑑𝑢 ∗ d𝑢 𝑑𝑥 𝑤 𝐼𝑛 𝑂𝑢𝑡 𝑤n 𝑎 𝑤m b 𝛿5 𝒚𝟏 𝒚𝟐 𝜕5 = 𝜕𝐸𝑟𝑟𝑜𝑟 𝜕𝑤5 = 𝜕𝐸𝑟𝑟𝑜𝑟 𝜕𝑂1 ∗ 𝜕𝑂1 𝜕𝐼𝑛𝑜1 ∗ 𝜕𝐼𝑛𝑜1 𝜕𝑤5 𝜕𝐸𝑟𝑟𝑜𝑟 𝜕𝑂1 =𝑜1 − 𝑦1 𝜕𝑂1 𝜕𝐼𝑛𝑜1 =𝑜1 ∗ (1 − 𝑜1) 𝜕𝐼𝑛𝑜1 𝜕𝑤5 =ℎ1 1. 计算𝑤1~ 𝑤4的梯度(以𝑤1为例) 𝑥1 𝑥2 ℎ1 ℎ2 𝑜1 𝑜2 𝑤1 𝑤2 𝑤3 𝑤5 𝑤6 𝑤7 𝑤8w4 反向传播——梯度计算 𝛿1 𝛿5 𝛿6 = 𝛿ℎ1 ∗ ℎ1 ∗ 1 − ℎ1 ∗ 𝑥1 𝛿1 = 𝜕𝐸𝑟𝑟𝑜𝑟 𝜕𝑤1 = 𝜕𝐸𝑟𝑟𝑜𝑟 𝜕ℎ1 ∗ 𝜕ℎ1 𝜕𝐼𝑛ℎ1 ∗ 𝜕𝐼𝑛ℎ1 𝜕𝑤1 更新参数，其中𝜂被称为学习率 𝑖𝑤′ = 𝑤𝑖− 𝜂∗ 𝛿𝑖 步长 传递 误差 原有 参数 参数更新 𝑥1 𝑥2 ℎ1 ℎ2 𝑜1 𝑜2 𝑤1 𝑤2 𝑤3 w4 𝑤5 𝑤6 𝑤7 𝑤8 𝛿o1 𝛿o2 𝛿5 𝛿6 𝛿7 𝛿8 𝛿1 𝛿2 𝛿3 𝛿4 神经网络的训练过程 初始化参数 训练次数=0 选取一部分 训练数据 通过前向传播 获得预测值 通过反向传播 更新参数 达到训 练目标？ 达到训 练次数？ 训练次数+1 结束训练 否 否 是 是 4. 基于Keras实现神经网络应用 Python的深度学习库Keras Keras 是深度学习框架，方便定义和训练各种深度学习模型。  高度模块化，使用简洁的API就可构建神经网络，支持快速开发。 并且支持CPU、GPU无缝切换运行。 注意：在Anaconda集成环境中安装Keras，同时需要安装后 端的深度学习引擎如Tensorflow： >>> pip install keras >>> pip install tensorflow  需要运行在专业的深度学习引擎之上， 如Tensorflow、CNTK和Theano等。 Keras的模块结构"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 10,
        "text": "𝛿o2 𝛿5 𝛿6 𝛿7 𝛿8 𝛿1 𝛿2 𝛿3 𝛿4 神经网络的训练过程 初始化参数 训练次数=0 选取一部分 训练数据 通过前向传播 获得预测值 通过反向传播 更新参数 达到训 练目标？ 达到训 练次数？ 训练次数+1 结束训练 否 否 是 是 4. 基于Keras实现神经网络应用 Python的深度学习库Keras Keras 是深度学习框架，方便定义和训练各种深度学习模型。  高度模块化，使用简洁的API就可构建神经网络，支持快速开发。 并且支持CPU、GPU无缝切换运行。 注意：在Anaconda集成环境中安装Keras，同时需要安装后 端的深度学习引擎如Tensorflow： >>> pip install keras >>> pip install tensorflow  需要运行在专业的深度学习引擎之上， 如Tensorflow、CNTK和Theano等。 Keras的模块结构 案例1：用深度神经网络实现鸢尾花分类 • Iris(鸢尾花) 是一个经典数据集，1936年Fisher在模式 识别论文中使用。 • 该数据集共150行，每行1个样本。 • 每个样本有4个特征：花萼长度、花萼宽度、花瓣长度、 花瓣宽度 • 类别(共3类)：山鸢尾Iris Setosa /变色鸢尾Iris Versicolour / 维吉尼亚鸢Iris Virginica Scikit-learn自带Iris数据集，可直接引入使用，也可使用下载的数据文件Iris.data。 编 号 花萼长 度(cm) 花萼宽 度(cm) 花瓣长 度(cm) 花瓣宽度 花的种类 1 5.1 3.5 1.4 0.2 山鸢尾 2 4.9 3.0 1.4 0.2 山鸢尾 3 7.0 3.2 4.7 1.4 杂色鸢尾 4 6.4 3.2 4.5 1.5 杂色鸢尾 5 6.3 3.3 6.0 2.5 维吉尼亚鸢尾 6 5.8 2.7 5.1 1.9 维吉尼亚鸢尾 神经网络分类程序实现方法 第一步导入Keras模型库，创建模型对象 Keras提供了两种“模型”来构建神经网络： Sequential：顺序式模型或序贯模型，可以通过各层按顺序线性堆叠来构建神经网络层 Functional：函数式模型，在顺序模型的基础上，允许多输出、共享层等结构 #导入Keras模型库，定义模型结构 from keras.mo"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 11,
        "text": "keras.layers import Dense, Dropout #导入层次库 # 通过堆叠层次来定义模型结构 model.add(Dense(16,activation=‘relu’,input_shape=(4,))) #隐层1 model.add(Dense(16,activation='relu')) #隐层2 model.add(Dropout(0.25)) #隐层2随机失活25% model.add(Dense(3,activation='softmax')) #输出层 输入层 （4维） 隐藏层1 (16维) 隐藏层2 （16维） 输出层 （3维） … … … … model.add(Dense(n, activation, input_shape)) Dense：全连接层，节点与下一层节点完全连接 n: 本层节点数 activation：激活函数，如有softmax、relu、tanh、sigmoid等 input_shape：输入数据维度，用元组表示如(6,8)，首层必需说明。 model.add(dropout) Dropout：在本层添加随机失活比例。为防止过拟合， 训练过程中断开一些输入神经元连接。 注意：多分类输出层的激活函数要 选择’softmax’，即返回一个由多个 概率值（总和为1）组成的数组， 每个概率值表示输出为某类的概率 from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(16, input_shape=(4,)), Activation('relu’), Dense(16), Activation('relu’), Dropout(0.25), Dense(3), Activation('softmax’), ]) 第三步 定义损失函数、优化器、性能评估指标等，并根据这些参数对网络进行编译。 编译之后的网络模型即可用于学习训练。 model.compile(loss, optimizer, metrics) loss：损失函数，如多分类用交叉熵损失‘categorical_crossentropy‘, 回归用均方损失 mean_squared_error等 optim"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 12,
        "text": "6.3 3.3 6.0 2.5 维吉尼亚鸢尾 6 5.8 2.7 5.1 1.9 维吉尼亚鸢尾 #特性X取值前4列(数据中无编号列) X = data.iloc[:,0:4].values.astype(float) #将类名转换为整数 data.loc[ data['class'] == 'Iris-setosa', 'class' ] = 0 data.loc[ data['class'] == 'Iris-versicolor', 'class' ] = 1 data.loc[ data['class'] == 'Iris-virginica', 'class' ] = 2 #标签y取值第4列 y = data.iloc[:,4].values.astype(int) #分割数据为训练集和测试集 train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0) #导入库，读入数据文件 import pandas as pd from sklearn.model_selection import train_test_split data = pd.read_csv('data\\iris.data', header = None) data.columns = ['sepal length','sepal width','petal length','petal width','class’] print(data.iloc[0:5,:]) #查看前5条数据 第四步 准备数据 对数据进行预处理，符合网络要求的类型、 形状和数据分布。 编 号 花萼长 度(cm) 花萼宽 度(cm) 花瓣长 度(cm) 花瓣宽 度(cm) 花的种类 1 5.1 3.5 1.4 0.2 山鸢尾 2 4.9 3.0 1.4 0.2 山鸢尾 3 7.0 3.2 4.7 1.4 杂色鸢尾 4 6.4 3.2 4.5 1.5 杂色鸢尾 5 6.3 3.3 6.0 2.5 维吉尼亚鸢尾 6 5.8 2.7 5.1 1.9 维吉尼亚鸢尾 2）标准化：将数据根据自身一定比例进行处理，使之落入一个小的特定 区间。因为取值范围差异大容易造成训练不收敛。 #特征"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 13,
        "text": "编 号 花萼长 度(cm) 花萼宽 度(cm) 花瓣长 度(cm) 花瓣宽 度(cm) 花的种类 1 5.1 3.5 1.4 0.2 山鸢尾 2 4.9 3.0 1.4 0.2 山鸢尾 3 7.0 3.2 4.7 1.4 杂色鸢尾 4 6.4 3.2 4.5 1.5 杂色鸢尾 5 6.3 3.3 6.0 2.5 维吉尼亚鸢尾 6 5.8 2.7 5.1 1.9 维吉尼亚鸢尾 3）类别标签独热编码：多分类使用损失函数 categorical_crossentropy，标签应该为多类模式， 即one-hot编码的向量，而不是单个数值。 #将标签的结果类型转化为one-hot独热矩阵 from keras.utils import np_utils train_y_ohe = np_utils.to_categorical(train_y, 3) test_y_ohe = np_utils.to_categorical(test_y, 3) print('前5条测试数据标签值：', test_y[0:5]) print('前5条测试数据标签的独热码：\\n',test_y_ohe[0:5]) np_utils提供函数实现将类别标签向量转换独热矩阵表示： from keras.utils import np_utils np_utils.to_categorical(类别标签, 总类别数) One-hot独热码是一组数，其中只有一个值为1，其余都是0。 如三类花的特征标签分别为0,1,2，采用独热编码，则将每个标签对应一个编码： 0->[1 0 0] 1->[0 1 0] 2->[0 0 1] 第五步 模型训练 #训练模型 model.fit(train_x, train_y_ohe, epochs=50, batch_size=1, verbose=2, validation_data=(test_x,test_y_ohe)) 各轮训练的情况： 时间、损失值、精确率、验证 集损失值、验证集精确率 model.fit( x, y, batch_size=32, epochs=10, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight="
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 14,
        "text": "= 1.0 注意：每次训练结果可能会不同。 loss,accuracy = model.evaluate(X_test,Y_test, verbose) loss：预测标签和目标标签之间的损失值。 accuracy：精确率 X_test ：测试集数据 Y_test ：测试集标签 verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录 model.predict(X_test, batch_size, verbose) X_test ：测试集数据 batch_size：整数，指定进行梯度下降时每个batch包含的样本数。 verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录。 Keras的符号计算 Keras的底层库使用Theano或TensorFlow，它们也称为Keras的后端。 Theano和TensorFlow，都是“符号式”的库。因此使得Keras的编程与传 统的Python代码有所差别。Keras采用符号计算。 符号计算流程： 1）首先定义各种变量，然后建立一个“计算图”， 计算图规定了各个变量之间的计算关系。 2）只有通过编译确定计算图的内部细节，或实际 生成可调用的函数后（K.function()），后面才能使用。 但这是仍是一个“空壳子”。 3）只有把需要运算的输入放进去后，才能在整个 模型中形成数据流，从而形成输出值。 计算图实例 K.function()是Keras的后端函数，它为符号计算实际生成可调用 的函数，它可以接收传入数据，并返回一个numpy数组。 5. 小结 神经网络建模步骤 数据准备 网络配置 模型训练 模型评估 模型预测 数据向量化、标准化、 独热标签 网络结构、损失函数、 优化器 训练、保存模型 观察模型训练的中间结 果和最后性能 加载模型，进行预测 数据向量化、标准化、 独热标签 定义model，编译model model.fit() model.evaluate() 输出cost，accuracy model.predict() Step1 Step2 Step3 Step4 Step5 神经网络算法的特点 缺点  BP算法学习速度慢 由于BP算法本质上为梯度下降法，而它所要优化的目标函数又非常复杂。  训练失败"
      },
      {
        "source": "1_神经网络_原理与实现.pdf",
        "chunk_id": 15,
        "text": "Dimensionality 784(28*28) Features integers 0-255 • 运行程序时，可以自动下载 • 但由于直接运行程序时下载数据较慢，可以先下载mnist.npz(约11M) 放到 ~/.keras/datasets文件夹下，win7/win8下: “~”文件夹一般是 C:\\Users\\Administrator 下载地址： 1）https://s3.amazonaws.com/img-datasets/mnist.npz 2）或其他镜像地址、用户分享等，如分享百度云： https://pan.baidu.com/s/1aZRp0uMkNj2QEWYstaNsKQ 提取码: 3a2u MNIST数据集 keras框架为我们提供了一些常用的内置数据集这些数据集可直接调用。 详见https://keras.io/zh/datasets/ 或“Keras数据集使用说明.doc”。 用给定大小的网格将连续图像离散化，每个小方格是一个像素（Pixel）， 对应一种颜色值，颜色值矩阵表示数字图像。例如分辨率是640×480，乘积就是 像素总数。同样大小的图像，像素越大越清晰。 补充提示1：图像的数字化表示  二值图像 像素矩阵由0、1两个值构成，”0”代表黑色，1”代白色。通常用于文字、 线条图的扫描识别（OCR）和掩膜图像的存储。 二值图像采用3D 张量表示： (samples, height, width)  灰度图像 灰度图像矩阵元素的取值范围通常为[0，255]。“0”表示纯黑色， “255”表示纯白色，中间的数字从小到大表示由黑到白的过渡色，每个 像素值用8位二进制表示。 二值图像可以看成是灰度图像的特例。 灰度图像采用3D 张量表示： (samples, height, width) 补充提示2：神经网络的数据表示 数据存储在多维数据中，也叫张量(tensor)。 • 仅包含一个数字的张量也叫标量(0D) • 一维数据叫向量，对应一维张量(1D) • 二维数据叫矩阵，对应二维张量(2D) • 将多个矩阵组合成一个新的数据，得到一个三维张量(3D) • 由3D张量组合可得到4维张量，以次类推，可得到高维张量。深度学习一般处 理0D~4D的张量。处理视频可能会遇到5D 可以通过python的以下属性，观察张量特性，例如： >>>x="
      }
    ]
  }
}
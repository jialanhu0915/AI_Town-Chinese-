"""
æµ‹è¯• DeepSeek-R1 æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›
éªŒè¯é…ç½®ç³»ç»Ÿå’Œæ¨¡å‹åˆ‡æ¢æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import os
import sys
from pathlib import Path

import pytest

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from ai_town.config_loader import get_current_llm_model, initialize_config
from ai_town.llm.llm_integration import ask_llm, chat_with_llm, setup_default_llm_providers


@pytest.mark.skipif(
    os.getenv("CI") == "true" or os.getenv("SKIP_OLLAMA_TESTS") == "true",
    reason="è·³è¿‡éœ€è¦Ollamaçš„æµ‹è¯•åœ¨CIç¯å¢ƒä¸­",
)
async def test_deepseek_model():
    """æµ‹è¯• DeepSeek-R1 æ¨¡å‹"""

    print("ğŸš€ DeepSeek-R1 æ¨¡å‹æµ‹è¯•")
    print("=" * 40)

    # åˆå§‹åŒ–é…ç½®
    initialize_config()
    setup_default_llm_providers()

    print(f"å½“å‰æ¨¡å‹: {get_current_llm_model()}")
    print()

    # æµ‹è¯• 1: ç®€å•é—®ç­”
    print("ğŸ“‹ æµ‹è¯• 1: ç®€å•é—®ç­”")
    try:
        response = await ask_llm("ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±", provider="ollama")
        print(f"DeepSeek-R1: {response}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

    print()

    # æµ‹è¯• 2: JSON æ ¼å¼å†³ç­–
    print("ğŸ“‹ æµ‹è¯• 2: JSON æ ¼å¼å†³ç­–")
    decision_prompt = """
ä½ æ˜¯Aliceï¼Œ32å²å’–å•¡åº—è€æ¿ã€‚ç°åœ¨æ˜¯ä¸Šåˆ10ç‚¹ï¼Œä½ åœ¨å’–å•¡åº—é‡Œã€‚

è¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªè¡Œä¸ºï¼š
1. work - å·¥ä½œ
2. socialize - ç¤¾äº¤  
3. think - æ€è€ƒ

ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼å›ç­”ï¼š
{"type": "work", "description": "æ³¡å’–å•¡", "reason": "ä¸Šåˆæ˜¯å¿™ç¢Œæ—¶é—´"}

JSONå“åº”ï¼š"""

    try:
        response = await ask_llm(decision_prompt, provider="ollama")
        print(f"DeepSeek-R1 åŸå§‹å›åº”: {response}")

        # æµ‹è¯•ä¼˜åŒ–çš„è§£æé€»è¾‘
        import json
        import re

        cleaned_response = response.strip()

        # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
        if "```json" in cleaned_response:
            json_match = re.search(r"```json\s*\n(.*?)\n```", cleaned_response, re.DOTALL)
            if json_match:
                cleaned_response = json_match.group(1)
        elif "```" in cleaned_response:
            json_match = re.search(r"```\s*\n(.*?)\n```", cleaned_response, re.DOTALL)
            if json_match:
                cleaned_response = json_match.group(1)

        try:
            parsed = json.loads(cleaned_response)
            print(f"âœ… JSON è§£ææˆåŠŸ: {parsed}")
        except json.JSONDecodeError:
            print(f"âš ï¸  JSON æ ¼å¼ä»éœ€ä¼˜åŒ–ï¼Œæ¸…ç†åå†…å®¹: {cleaned_response}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

    print()

    # æµ‹è¯• 3: è§’è‰²å¯¹è¯
    print("ğŸ“‹ æµ‹è¯• 3: è§’è‰²å¯¹è¯")
    conversation_messages = [
        {"role": "system", "content": "ä½ æ˜¯Aliceï¼Œå‹å¥½çš„å’–å•¡åº—è€æ¿ã€‚"},
        {"role": "system", "content": "è¯·ç”¨1å¥è¯ç®€çŸ­å›åº”ï¼ˆä¸è¶…è¿‡20å­—ï¼‰ã€‚"},
        {"role": "user", "content": "Bob: ä½ å¥½Aliceï¼Œä»Šå¤©å’–å•¡é¦™å‘³ç‰¹åˆ«æµ“å‘¢ï¼"},
    ]

    try:
        response = await chat_with_llm(conversation_messages, provider="ollama")
        print(f"Alice (DeepSeek-R1): {response}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

    print()

    # æµ‹è¯• 4: ä¸­æ–‡å¯¹è¯æµç•…æ€§
    print("ğŸ“‹ æµ‹è¯• 4: ä¸­æ–‡å¯¹è¯æµç•…æ€§")
    chinese_messages = [
        {"role": "system", "content": "ä½ æ˜¯Charlieï¼Œ28å²ä¸Šç­æ—ï¼Œåˆšæ¬åˆ°é•‡ä¸Šã€‚"},
        {"role": "system", "content": "ç”¨1å¥è¯å›åº”ï¼Œä½“ç°é€‚åº”æ–°ç¯å¢ƒçš„å¿ƒæ€ã€‚"},
        {"role": "user", "content": "Alice: æ¬¢è¿æ¥åˆ°æˆ‘ä»¬å°é•‡ï¼ä½ è§‰å¾—è¿™é‡Œæ€ä¹ˆæ ·ï¼Ÿ"},
    ]

    try:
        response = await chat_with_llm(chinese_messages, provider="ollama")
        print(f"Charlie (DeepSeek-R1): {response}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

    print("\nğŸ‰ DeepSeek-R1 æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    asyncio.run(test_deepseek_model())
